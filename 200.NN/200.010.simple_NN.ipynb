{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47daf5f3-ee39-45cc-9369-6e55e3a0623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データの生成\n",
    "# 1次元のデータを生成します (x: 0から1のランダム値, y: sin(2 * pi * x) + ノイズ)\n",
    "np.random.seed(0)  # 再現性のために乱数シードを固定\n",
    "x = np.random.rand(100, 1)  # 100個のサンプルを生成\n",
    "y = np.sin(2 * np.pi * x) + 0.1 * np.random.randn(100, 1)  # ノイズを加えた出力\n",
    "\n",
    "# numpy配列をPyTorchのテンソルに変換\n",
    "X_train = torch.FloatTensor(x)\n",
    "y_train = torch.FloatTensor(y)\n",
    "\n",
    "# ニューラルネットワークモデルの定義\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.hidden = nn.Linear(1, 10)  # 隠れ層 (入力次元: 1, 出力次元: 10)\n",
    "        self.relu = nn.ReLU()            # 活性化関数\n",
    "        self.output = nn.Linear(10, 1)   # 出力層 (入力次元: 10, 出力次元: 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# モデル、損失関数、最適化手法の初期化\n",
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()  # 平均二乗誤差損失関数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adamオプティマイザ\n",
    "\n",
    "# 学習ループ\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # モデルを訓練モードにセット\n",
    "    optimizer.zero_grad()  # 勾配を初期化\n",
    "\n",
    "    # フォワードパス\n",
    "    predictions = model(X_train)\n",
    "\n",
    "    # 損失計算\n",
    "    loss = criterion(predictions, y_train)\n",
    "\n",
    "    # バックプロパゲーション\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 進捗の表示\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# モデルの評価\n",
    "model.eval()  # モデルを評価モードにセット\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_train).numpy()\n",
    "\n",
    "# 結果のプロット\n",
    "plt.scatter(x, y, color='blue', label='Data')\n",
    "plt.plot(x, y_pred, color='red', label='Predictions', linewidth=2)\n",
    "plt.title('Regression using Neural Network')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b2d8f5-32c4-445e-bd59-a51463aa87d2",
   "metadata": {},
   "source": [
    "上のコードは図示部分がおかしい。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c69c9-3a64-4f61-b109-eaf8a4ef99ca",
   "metadata": {},
   "source": [
    "修正後"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b168cc-36e4-4c78-91ca-e98e99ac6c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データの生成\n",
    "# 1次元のデータを生成します (x: 0から1のランダム値, y: sin(2 * pi * x) + ノイズ)\n",
    "np.random.seed(0)  # 再現性のために乱数シードを固定\n",
    "x = np.random.rand(100, 1)  # 100個のサンプルを生成\n",
    "y = np.sin(2 * np.pi * x) + 0.1 * np.random.randn(100, 1)  # ノイズを加えた出力\n",
    "\n",
    "# numpy配列をPyTorchのテンソルに変換\n",
    "X_train = torch.FloatTensor(x)\n",
    "y_train = torch.FloatTensor(y)\n",
    "\n",
    "# ニューラルネットワークモデルの定義\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.hidden = nn.Linear(1, 10)  # 隠れ層 (入力次元: 1, 出力次元: 10)\n",
    "        self.relu = nn.ReLU()            # 活性化関数\n",
    "        self.output = nn.Linear(10, 1)   # 出力層 (入力次元: 10, 出力次元: 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# モデル、損失関数、最適化手法の初期化\n",
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()  # 平均二乗誤差損失関数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adamオプティマイザ\n",
    "\n",
    "# 学習ループ\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # モデルを訓練モードにセット\n",
    "    optimizer.zero_grad()  # 勾配を初期化\n",
    "\n",
    "    # フォワードパス\n",
    "    predictions = model(X_train)\n",
    "\n",
    "    # 損失計算\n",
    "    loss = criterion(predictions, y_train)\n",
    "\n",
    "    # バックプロパゲーション\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 進捗の表示\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# モデルの評価\n",
    "model.eval()  # モデルを評価モードにセット\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_train).numpy()\n",
    "\n",
    "# 結果のプロット\n",
    "plt.scatter(y,y_pred, color='blue', label='Data')\n",
    "plt.title('Regression using Neural Network')\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('y^pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a2558-05f5-43ea-83c9-f2f4bca8773c",
   "metadata": {},
   "source": [
    "ドロップアウト、バッチ正則化を有効に"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d754d3e-74fa-4b96-85f8-ccc105c1c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データの生成\n",
    "np.random.seed(0)  # 再現性のために乱数シードを固定\n",
    "x = np.random.rand(100, 1)  # 100個のサンプルを生成\n",
    "y = np.sin(2 * np.pi * x) + 0.1 * np.random.randn(100, 1)  # ノイズを加えた出力\n",
    "\n",
    "# numpy配列をPyTorchのテンソルに変換\n",
    "X_train = torch.FloatTensor(x)\n",
    "y_train = torch.FloatTensor(y)\n",
    "\n",
    "# ニューラルネットワークモデルの定義\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.hidden = nn.Linear(1, 10)  # 隠れ層\n",
    "        self.batch_norm = nn.BatchNorm1d(10)  # バッチ正則化\n",
    "        self.relu = nn.ReLU()            # 活性化関数\n",
    "        self.dropout = nn.Dropout(0.2)   # ドロップアウト (50%の確率でニューロンを無効化)\n",
    "        self.output = nn.Linear(10, 1)   # 出力層\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)               # 隠れ層へのパス\n",
    "        x = self.batch_norm(x)           # バッチ正則化\n",
    "        x = self.relu(x)                 # 活性化関数の適用\n",
    "        x = self.dropout(x)               # ドロップアウトの適用\n",
    "        x = self.output(x)               # 出力層へのパス\n",
    "        return x\n",
    "\n",
    "# モデル、損失関数、最適化手法の初期化\n",
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()              # 平均二乗誤差損失関数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adamオプティマイザ\n",
    "\n",
    "# 学習ループ\n",
    "num_epochs = 3000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # モデルを訓練モードにセット\n",
    "    optimizer.zero_grad()  # 勾配を初期化\n",
    "\n",
    "    # フォワードパス\n",
    "    predictions = model(X_train)\n",
    "\n",
    "    # 損失計算\n",
    "    loss = criterion(predictions, y_train)\n",
    "\n",
    "    # バックプロパゲーション\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 進捗の表示\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# モデルの評価\n",
    "model.eval()  # モデルを評価モードにセット\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_train).numpy()\n",
    "\n",
    "# 結果のプロット\n",
    "plt.scatter(y, y_pred, color='blue', label='Data')\n",
    "plt.title('Regression using Neural Network with Dropout and Batch Normalization')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c386657-974e-4163-bc65-53c430b39ca5",
   "metadata": {},
   "source": [
    "```\n",
    "# 依頼\n",
    "コードを修正してください。\n",
    "# 詳細\n",
    "ドロップアウトとバッチ正則化を無効します。\n",
    "全データを訓練データとテストデータに分けます。\n",
    "epoch毎に損失関数の値を図示してください。\n",
    "テストデータで回帰モデルの評価指標MAE，R２，RMSEを出力してください。\n",
    "y vs y_predの図を書いてください。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3e8da5-1dbd-462e-bf00-7172c98bc84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "# データの生成\n",
    "np.random.seed(0)  # 再現性のために乱数シードを固定\n",
    "x = np.random.rand(100, 1)  # 100個のサンプルを生成\n",
    "y = np.sin(2 * np.pi * x) + 0.1 * np.random.randn(100, 1)  # ノイズを加えた出力\n",
    "\n",
    "# numpy配列をPyTorchのテンソルに変換\n",
    "X = torch.FloatTensor(x)\n",
    "y = torch.FloatTensor(y)\n",
    "\n",
    "# データの分割 (80%を訓練データ、20%をテストデータ)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# ニューラルネットワークモデルの定義\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.hidden = nn.Linear(1, 10)  # 隠れ層\n",
    "        self.relu = nn.ReLU()            # 活性化関数\n",
    "        self.output = nn.Linear(10, 1)   # 出力層\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)               # 隠れ層へのパス\n",
    "        x = self.relu(x)                 # 活性化関数の適用\n",
    "        x = self.output(x)               # 出力層へのパス\n",
    "        return x\n",
    "\n",
    "# モデル、損失関数、最適化手法の初期化\n",
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()              # 平均二乗誤差損失関数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adamオプティマイザ\n",
    "\n",
    "# 学習ループ\n",
    "num_epochs = 1000\n",
    "losses = []  # 損失を保持するリスト\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # モデルを訓練モードにセット\n",
    "    optimizer.zero_grad()  # 勾配を初期化\n",
    "\n",
    "    # フォワードパス\n",
    "    predictions = model(X_train)\n",
    "\n",
    "    # 損失計算\n",
    "    loss = criterion(predictions, y_train)\n",
    "    losses.append(loss.item())  # 損失をリストに追加\n",
    "\n",
    "    # バックプロパゲーション\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 進捗の表示\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# モデルの評価\n",
    "model.eval()  # モデルを評価モードにセット\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test).numpy()  # テストデータに対する予測\n",
    "\n",
    "# エポックごとに損失の値を図示\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(num_epochs), losses, label='Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 評価指標の計算\n",
    "mae = mean_absolute_error(y_test.numpy(), y_pred)\n",
    "r2 = r2_score(y_test.numpy(), y_pred)\n",
    "rmse = mean_squared_error(y_test.numpy(), y_pred, squared=False)\n",
    "\n",
    "print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "print(f'R² Score: {r2:.4f}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
    "\n",
    "# y vs y_predの図を作成\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(y_test.numpy(), y_pred, color='blue', label='Predicted vs Actual')\n",
    "plt.plot([min(y_test.numpy()), max(y_test.numpy())], \n",
    "         [min(y_test.numpy()), max(y_test.numpy())], \n",
    "         color='red', label='Ideal Prediction', linewidth=2)\n",
    "plt.title('True vs Predicted Values')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aa72c9-f30e-4409-94e0-95a59e433dfd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```\n",
    "# 依頼\n",
    "コードを修正してください。\n",
    "# 詳細\n",
    "説明変数が５つのデータを用いてください。\n",
    "ドロップアウトを有効にします。\n",
    "全データを訓練データとテストデータに分けます。\n",
    "epoch毎に損失関数の値を図示してください。\n",
    "テストデータで回帰モデルの評価指標MAE，R２，RMSEを出力してください。\n",
    "y vs y_predの図を書いてください。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa09ab6-22d4-41b6-86eb-a0ec67b6793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "# データの生成\n",
    "np.random.seed(0)  # 再現性のために乱数シードを固定\n",
    "X = np.random.rand(100, 5)  # 100個のサンプル、5つの特徴量\n",
    "y = np.sin(2 * np.pi * X[:, 0]) + 0.1 * np.random.randn(100)  # ノイズを加えた出力 (1つの特徴量による)\n",
    "\n",
    "# numpy配列をPyTorchのテンソルに変換\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.FloatTensor(y).view(-1, 1)\n",
    "\n",
    "# データの分割 (80%を訓練データ、20%をテストデータ)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=0)\n",
    "\n",
    "# ニューラルネットワークモデルの定義\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.hidden = nn.Linear(5, 10)  # 隠れ層 (5つの特徴量を入力)\n",
    "        self.relu = nn.ReLU()            # 活性化関数\n",
    "        self.dropout = nn.Dropout(0.5)   # ドロップアウト (50%の確率でニューロンを無効化)\n",
    "        self.output = nn.Linear(10, 1)   # 出力層\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)               # 隠れ層へのパス\n",
    "        x = self.relu(x)                 # 活性化関数の適用\n",
    "        x = self.dropout(x)              # ドロップアウトの適用\n",
    "        x = self.output(x)               # 出力層へのパス\n",
    "        return x\n",
    "\n",
    "# モデル、損失関数、最適化手法の初期化\n",
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()              # 平均二乗誤差損失関数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adamオプティマイザ\n",
    "\n",
    "# 学習ループ\n",
    "num_epochs = 1000\n",
    "losses = []  # 損失を保持するリスト\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # モデルを訓練モードにセット\n",
    "    optimizer.zero_grad()  # 勾配を初期化\n",
    "\n",
    "    # フォワードパス\n",
    "    predictions = model(X_train)\n",
    "\n",
    "    # 損失計算\n",
    "    loss = criterion(predictions, y_train)\n",
    "    losses.append(loss.item())  # 損失をリストに追加\n",
    "\n",
    "    # バックプロパゲーション\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 進捗の表示\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# モデルの評価\n",
    "model.eval()  # モデルを評価モードにセット\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test).numpy()  # テストデータに対する予測\n",
    "\n",
    "# エポックごとに損失の値を図示\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(num_epochs), losses, label='Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 評価指標の計算\n",
    "mae = mean_absolute_error(y_test.numpy(), y_pred)\n",
    "r2 = r2_score(y_test.numpy(), y_pred)\n",
    "rmse = mean_squared_error(y_test.numpy(), y_pred, squared=False)\n",
    "\n",
    "print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "print(f'R² Score: {r2:.4f}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
    "\n",
    "# y vs y_predの図を作成\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(y_test.numpy(), y_pred, color='blue', label='Predicted vs Actual')\n",
    "plt.plot([min(y_test.numpy()), max(y_test.numpy())], \n",
    "         [min(y_test.numpy()), max(y_test.numpy())], \n",
    "         color='red', label='Ideal Prediction', linewidth=2)\n",
    "plt.title('True vs Predicted Values')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f92bd8-9f31-4534-b9bf-397884a62724",
   "metadata": {},
   "source": [
    "# GPUを使うように書き直してください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97cc713-7c25-4b0c-bb48-961348d6c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "# データの生成\n",
    "np.random.seed(0)  # 再現性のために乱数シードを固定\n",
    "X = np.random.rand(100, 5)  # 100個のサンプル、5つの特徴量\n",
    "y = np.sin(2 * np.pi * X[:, 0]) + 0.1 * np.random.randn(100)  # ノイズを加えた出力\n",
    "\n",
    "# numpy配列をPyTorchのテンソルに変換\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.FloatTensor(y).view(-1, 1)\n",
    "\n",
    "# データの分割 (80%を訓練データ、20%をテストデータ)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=0)\n",
    "\n",
    "# GPUの使用を確認\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# データをGPUに転送\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# ニューラルネットワークモデルの定義\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.hidden = nn.Linear(5, 10)  # 隠れ層 (5つの特徴量を入力)\n",
    "        self.relu = nn.ReLU()            # 活性化関数\n",
    "        self.dropout = nn.Dropout(0.5)   # ドロップアウト (50%の確率でニューロンを無効化)\n",
    "        self.output = nn.Linear(10, 1)   # 出力層\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)               # 隠れ層へのパス\n",
    "        x = self.relu(x)                 # 活性化関数の適用\n",
    "        x = self.dropout(x)              # ドロップアウトの適用\n",
    "        x = self.output(x)               # 出力層へのパス\n",
    "        return x\n",
    "\n",
    "# モデルをGPUに転送\n",
    "model = SimpleNN().to(device)\n",
    "\n",
    "# 損失関数と最適化手法の初期化\n",
    "criterion = nn.MSELoss()              # 平均二乗誤差損失関数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adamオプティマイザ\n",
    "\n",
    "# 学習ループ\n",
    "num_epochs = 1000\n",
    "losses = []  # 損失を保持するリスト\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # モデルを訓練モードにセット\n",
    "    optimizer.zero_grad()  # 勾配を初期化\n",
    "\n",
    "    # フォワードパス\n",
    "    predictions = model(X_train)\n",
    "\n",
    "    # 損失計算\n",
    "    loss = criterion(predictions, y_train)\n",
    "    losses.append(loss.item())  # 損失をリストに追加\n",
    "\n",
    "    # バックプロパゲーション\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 進捗の表示\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# モデルの評価\n",
    "model.eval()  # モデルを評価モードにセット\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test).cpu().numpy()  # テストデータに対する予測をCPUへ転送してNumPyに変換\n",
    "\n",
    "# エポックごとに損失の値を図示\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(num_epochs), losses, label='Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 評価指標の計算\n",
    "mae = mean_absolute_error(y_test.cpu().numpy(), y_pred)\n",
    "r2 = r2_score(y_test.cpu().numpy(), y_pred)\n",
    "rmse = mean_squared_error(y_test.cpu().numpy(), y_pred, squared=False)\n",
    "\n",
    "print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "print(f'R² Score: {r2:.4f}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
    "\n",
    "# y vs y_predの図を作成\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(y_test.cpu().numpy(), y_pred, color='blue', label='Predicted vs Actual')\n",
    "plt.plot([min(y_test.cpu().numpy()), max(y_test.cpu().numpy())], \n",
    "         [min(y_test.cpu().numpy()), max(y_test.cpu().numpy())], \n",
    "         color='red', label='Ideal Prediction', linewidth=2)\n",
    "plt.title('True vs Predicted Values')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5343d173-4e89-40d8-98ae-c8ee20b85db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91cf26bd-5c73-4040-bf24-c787c8b39951",
   "metadata": {},
   "source": [
    "# 100.regression/で用いたReCoデータを用いる。\n",
    "\n",
    "- 入力層のサイズをPとする。\n",
    "- 隠れ層を２層にする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b1da02-b7db-4ca5-890d-77a252d37c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_data\n",
    "DATA_NAME = \"ReCo\"\n",
    "df_obs,descriptor_names, target_name = get_data.load(DATA_NAME, \"..\")\n",
    "Xraw = df_obs.loc[:, descriptor_names].values\n",
    "y = df_obs.loc[:, target_name].values\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(Xraw)\n",
    "P = X.shape[1]\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c2611-29fa-4419-b296-cae7a10b7882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "# データの生成\n",
    "# np.random.seed(0)  # 再現性のために乱数シードを固定\n",
    "# X = np.random.rand(100, 5)  # 100個のサンプル、5つの特徴量\n",
    "# y = np.sin(2 * np.pi * X[:, 0]) + 0.1 * np.random.randn(100)  # ノイズを加えた出力\n",
    "\n",
    "# numpy配列をPyTorchのテンソルに変換\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.FloatTensor(y).view(-1, 1)\n",
    "\n",
    "# データの分割 (80%を訓練データ、20%をテストデータ)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=0)\n",
    "\n",
    "# GPUの使用を確認\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# データをGPUに転送\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# ---------書き換え部分\n",
    "# ニューラルネットワークモデルの定義\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, P):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(P, 10)  # 隠れ層1\n",
    "        self.hidden2 = nn.Linear(10, 5)  # 隠れ層2\n",
    "        self.output = nn.Linear(5, 1)  # 出力層\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden1(x))\n",
    "        x = torch.relu(self.hidden2(x))\n",
    "        return self.output(x)\n",
    "\n",
    "# モデルをGPUに転送\n",
    "model = SimpleNN(P).to(device)\n",
    "# ---------書き換え部分ここまで\n",
    "\n",
    "# 損失関数と最適化手法の初期化\n",
    "criterion = nn.MSELoss()              # 平均二乗誤差損失関数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adamオプティマイザ\n",
    "\n",
    "# 学習ループ\n",
    "num_epochs = 1000\n",
    "losses = []  # 損失を保持するリスト\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # モデルを訓練モードにセット\n",
    "    optimizer.zero_grad()  # 勾配を初期化\n",
    "\n",
    "    # フォワードパス\n",
    "    predictions = model(X_train)\n",
    "\n",
    "    # 損失計算\n",
    "    loss = criterion(predictions, y_train)\n",
    "    losses.append(loss.item())  # 損失をリストに追加\n",
    "\n",
    "    # バックプロパゲーション\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 進捗の表示\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# モデルの評価\n",
    "model.eval()  # モデルを評価モードにセット\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test).cpu().numpy()  # テストデータに対する予測をCPUへ転送してNumPyに変換\n",
    "\n",
    "# エポックごとに損失の値を図示\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(num_epochs), losses, label='Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 評価指標の計算\n",
    "mae = mean_absolute_error(y_test.cpu().numpy(), y_pred)\n",
    "r2 = r2_score(y_test.cpu().numpy(), y_pred)\n",
    "rmse = mean_squared_error(y_test.cpu().numpy(), y_pred, squared=False)\n",
    "\n",
    "print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "print(f'R² Score: {r2:.4f}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
    "\n",
    "# y vs y_predの図を作成\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(y_test.cpu().numpy(), y_pred, color='blue', label='Predicted vs Actual')\n",
    "plt.plot([min(y_test.cpu().numpy()), max(y_test.cpu().numpy())], \n",
    "         [min(y_test.cpu().numpy()), max(y_test.cpu().numpy())], \n",
    "         color='red', label='Ideal Prediction', linewidth=2)\n",
    "plt.title('True vs Predicted Values')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0335fe-4c21-4252-8757-6cacf811f878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
